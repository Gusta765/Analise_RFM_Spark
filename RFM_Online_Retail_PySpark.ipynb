{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise RFM (Recency, Frequency, Monetary) com Apache Spark (PySpark)\n",
    "\n",
    "**Dataset:** Online Retail (UCI Machine Learning Repository)  \n",
    "Fonte: https://archive.ics.uci.edu/ml/machine-learning-databases/00352/Online%20Retail.xlsx\n",
    "\n",
    "**Objetivo:** calcular métricas RFM por cliente, criar scores (1 a 5) com `ntile(5)` e segmentar clientes para apoiar decisões de CRM e retenção.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introdução\n",
    "\n",
    "RFM é uma técnica de segmentação de clientes baseada em três dimensões:\n",
    "\n",
    "- **Recency (R):** quão recente foi a última compra (quanto menor, melhor).\n",
    "- **Frequency (F):** número de compras/pedidos (quanto maior, melhor).\n",
    "- **Monetary (M):** valor total gasto (quanto maior, melhor).\n",
    "\n",
    "O dataset **Online Retail** contém transações de uma empresa de varejo online (invoices, produtos, quantidade, preço, país e `CustomerID`).\n",
    "\n",
    "Neste notebook:\n",
    "\n",
    "- A **limpeza e filtragem** será feita **exclusivamente em Spark SQL**.\n",
    "- O **cálculo do RFM** será feito com **PySpark DataFrame API**.\n",
    "- O **scoring** será feito com **window functions** usando **`ntile(5)`**.\n",
    "\n",
    "Observação sobre lazy evaluation: transformações são definidas de forma declarativa; ações como `show()` e `count()` disparam a execução.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup do Ambiente Spark\n",
    "\n",
    "Notebook pronto para rodar em **Google Colab** com Spark local via PySpark.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install pyspark==3.5.1 openpyxl==3.1.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName('RFM - Online Retail')\n",
    "    .master('local[*]')\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel('WARN')\n",
    "print('Spark version:', spark.version)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Ingestão dos Dados\n",
    "\n",
    "O dataset original está em **Excel (.xlsx)**. Como a ingestão em Spark será via **CSV**, vamos:\n",
    "\n",
    "1. Baixar o `.xlsx` da UCI\n",
    "2. Converter para `.csv` **sem Pandas** (somente `openpyxl` + `csv`)\n",
    "3. Ler o CSV com `spark.read.csv`\n",
    "\n",
    "Nesta etapa não aplicamos regras de negócio; apenas preparamos o formato de leitura.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import requests\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "base_dir = '/content'\n",
    "xlsx_path = os.path.join(base_dir, 'OnlineRetail.xlsx')\n",
    "csv_path = os.path.join(base_dir, 'OnlineRetail.csv')\n",
    "\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00352/Online%20Retail.xlsx'\n",
    "\n",
    "if not os.path.exists(xlsx_path):\n",
    "    resp = requests.get(url, timeout=120)\n",
    "    resp.raise_for_status()\n",
    "    with open(xlsx_path, 'wb') as f:\n",
    "        f.write(resp.content)\n",
    "\n",
    "if not os.path.exists(csv_path):\n",
    "    wb = load_workbook(xlsx_path, read_only=True, data_only=True)\n",
    "    ws = wb.active\n",
    "    with open(csv_path, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        for row in ws.iter_rows(values_only=True):\n",
    "            writer.writerow(list(row))\n",
    "    wb.close()\n",
    "\n",
    "print('CSV pronto em:', csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = (\n",
    "    spark.read\n",
    "    .option('header', True)\n",
    "    .csv(csv_path)\n",
    ")\n",
    "\n",
    "df_raw.printSchema()\n",
    "df_raw.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Limpeza e Regras de Negócio (Spark SQL)\n",
    "\n",
    "Nesta seção, a limpeza e filtragem são feitas **exclusivamente em Spark SQL**.\n",
    "\n",
    "Regras obrigatórias:\n",
    "\n",
    "- Remover registros com `CustomerID` nulo\n",
    "- Remover cancelamentos (`InvoiceNo` iniciando com `C`)\n",
    "- Remover quantidades `<= 0`\n",
    "- Criar `Revenue = Quantity * UnitPrice`\n",
    "\n",
    "Saída: view temporária **`vendas_clean`**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50d27d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.createOrReplaceTempView('vendas_raw')\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "Create Or Replace Temp View vendas_clean AS\n",
    "Select\n",
    "  Cast(InvoiceNo AS String)                            AS InvoiceNo,\n",
    "  Cast(StockCode AS String)                            AS StockCode,\n",
    "  Cast(Description AS String)                          AS Description,\n",
    "  Cast(Quantity AS Int)                                AS Quantity,\n",
    "  Cast(UnitPrice AS Double)                            AS UnitPrice,\n",
    "  Cast(Cast(CustomerID AS Double) AS Int)              AS CustomerID,\n",
    "  Cast(Country AS String)                              AS Country,\n",
    "  Cast(Quantity AS Double) * Cast(UnitPrice AS Double) AS Revenue,\n",
    "  Coalesce(\n",
    "    to_timestamp(InvoiceDate),\n",
    "    to_timestamp(InvoiceDate, 'M/d/yyyy H:mm')\n",
    "  )                                                     AS InvoiceDate\n",
    "From \n",
    "  vendas_raw\n",
    "Where 1 = 1\n",
    "  And CustomerID Is Not Null\n",
    "  And InvoiceNo Not Like 'C%'\n",
    "  And Cast(Quantity AS Int) > 0\n",
    "  And Cast(UnitPrice AS Double) > 0\n",
    "\"\"\")\n",
    "\n",
    "df_clean = spark.table('vendas_clean')\n",
    "df_clean.printSchema()\n",
    "df_clean.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896ca403",
   "metadata": {},
   "source": [
    "**Cache para evitar recomputação durante múltiplas agregações.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e32169",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.cache()\n",
    "print('Linhas em vendas_clean:', df_clean.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f4d2f8",
   "metadata": {},
   "source": [
    "## 5. Construção do RFM (PySpark)\n",
    "\n",
    "A partir de `vendas_clean`, calculamos por `CustomerID`:\n",
    "\n",
    "- **Recency:** dias desde a última compra\n",
    "- **Frequency:** número distinto de `InvoiceNo`\n",
    "- **Monetary:** soma da receita (`Revenue`)\n",
    "\n",
    "Data de referência: `max(InvoiceDate) + 1 dia`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6341b4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "max_invoice_ts = df_clean.select(F.max('InvoiceDate').alias('max_invoice_ts')).first()['max_invoice_ts']\n",
    "reference_date = max_invoice_ts + timedelta(days=1)\n",
    "\n",
    "rfm = (\n",
    "    df_clean\n",
    "    .groupBy('CustomerID')\n",
    "    .agg(\n",
    "        F.datediff(F.lit(reference_date), F.max('InvoiceDate')).alias('Recency'),\n",
    "        F.countDistinct('InvoiceNo').alias('Frequency'),\n",
    "        F.sum('Revenue').alias('Monetary')\n",
    "    )\n",
    ")\n",
    "\n",
    "rfm.show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82736bd",
   "metadata": {},
   "source": [
    "## 6. Scoring RFM (Window + `ntile(5)`)\n",
    "\n",
    "Scores de 1 a 5:\n",
    "\n",
    "- `R_Score`: quanto mais recente, maior o score\n",
    "- `F_Score`: quanto mais frequente, maior o score\n",
    "- `M_Score`: quanto maior o gasto, maior o score\n",
    "\n",
    "O `RFM_Score` será a concatenação `R_ScoreF_ScoreM_Score` (ex.: `555`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7f14d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_r = Window.orderBy(F.col('Recency').asc())\n",
    "w_f = Window.orderBy(F.col('Frequency').desc())\n",
    "w_m = Window.orderBy(F.col('Monetary').desc())\n",
    "\n",
    "rfm_scored = (\n",
    "    rfm\n",
    "    .withColumn('R_Score', F.lit(6) - F.ntile(5).over(w_r))\n",
    "    .withColumn('F_Score', F.lit(6) - F.ntile(5).over(w_f))\n",
    "    .withColumn('M_Score', F.lit(6) - F.ntile(5).over(w_m))\n",
    "    .withColumn('RFM_Score', F.concat_ws('', F.col('R_Score'), F.col('F_Score'), F.col('M_Score')))\n",
    ")\n",
    "\n",
    "rfm_scored.orderBy(F.desc('RFM_Score')).show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c25a90",
   "metadata": {},
   "source": [
    "**Plano lógico e físico otimizado pelo Catalyst.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecf1ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfm_scored.explain(True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Segmentação de Clientes\n",
    "\n",
    "Segmentos (heurísticos) baseados nos scores:\n",
    "\n",
    "- **Champions:** muito recentes, frequentes e com alto gasto\n",
    "- **Loyal Customers:** alta frequência e boa recência\n",
    "- **Big Spenders:** alto gasto (mesmo que frequência seja moderada)\n",
    "- **At Risk:** pouca recência (tempo sem comprar) porém já compraram com alguma frequência\n",
    "\n",
    "As regras abaixo são um ponto de partida e devem ser calibradas conforme a estratégia do negócio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmented = (\n",
    "    rfm_scored\n",
    "    .withColumn(\n",
    "        'Segment',\n",
    "        F.when((F.col('R_Score') >= 4) & (F.col('F_Score') >= 4) & (F.col('M_Score') >= 4), 'Champions')\n",
    "         .when((F.col('R_Score') >= 3) & (F.col('F_Score') >= 4), 'Loyal Customers')\n",
    "         .when((F.col('M_Score') >= 4) & (F.col('R_Score') >= 3), 'Big Spenders')\n",
    "         .when((F.col('R_Score') <= 2) & (F.col('F_Score') >= 3), 'At Risk')\n",
    "         .otherwise('Others')\n",
    "    )\n",
    ")\n",
    "\n",
    "segmented.groupBy('Segment').count().orderBy(F.desc('count')).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Análise de Resultados\n",
    "\n",
    "Vamos calcular:\n",
    "\n",
    "- Quantidade de clientes por segmento\n",
    "- Receita total e ticket médio por segmento\n",
    "\n",
    "Essas visões ajudam a priorizar ações (retenção, reativação e upsell).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clientes_por_segmento = (\n",
    "    segmented\n",
    "    .groupBy('Segment')\n",
    "    .agg(F.count('*').alias('Clientes'))\n",
    "    .orderBy(F.desc('Clientes'))\n",
    ")\n",
    "\n",
    "receita_por_segmento = (\n",
    "    segmented\n",
    "    .groupBy('Segment')\n",
    "    .agg(\n",
    "        F.sum('Monetary').alias('Receita_Total'),\n",
    "        F.avg('Monetary').alias('Ticket_Medio')\n",
    "    )\n",
    "    .orderBy(F.desc('Receita_Total'))\n",
    ")\n",
    "\n",
    "clientes_por_segmento.show(truncate=False)\n",
    "receita_por_segmento.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights de negócio (direcionamento)\n",
    "\n",
    "- **Champions:** campanhas de retenção premium, early access, programas VIP.\n",
    "- **Loyal Customers:** recomendação personalizada e bundles para elevar ticket médio.\n",
    "- **Big Spenders:** ofertas premium e upsell; foco em experiência e atendimento.\n",
    "- **At Risk:** campanhas de reativação (cupom, frete, comunicação multicanal) e pesquisa de motivos de churn.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusão\n",
    "\n",
    "Principais aprendizados:\n",
    "\n",
    "- RFM resume comportamento de compra em métricas simples e interpretáveis.\n",
    "- O scoring via quantis (`ntile`) permite comparar clientes de forma relativa.\n",
    "- A segmentação direciona ações com maior potencial de impacto.\n",
    "\n",
    "Próximos passos sugeridos:\n",
    "\n",
    "- Clusterização (ex.: K-Means) em features RFM padronizadas\n",
    "- CLV (Customer Lifetime Value)\n",
    "- Modelos preditivos (propensão à recompra/churn)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "RFM_Online_Retail_PySpark.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
